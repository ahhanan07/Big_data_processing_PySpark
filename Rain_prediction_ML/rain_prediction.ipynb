{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries for all the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col # importing functions isnan, when, count, col\n",
    "import pyspark.sql.functions as F # importing function library\n",
    "from pyspark.sql.types import DoubleType # importing doubletype used for column datatype changes\n",
    "from pyspark.sql import SparkSession # importing spark session to be used to initiate nodes\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler,OneHotEncoder#importing encoding techniques\n",
    "from pyspark.ml import Pipeline # pipeline method to be used for ML algorithms\n",
    "from pyspark.ml.classification import LogisticRegression # importing logistic model\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator # importing binary classification for metric evaluation\n",
    "from pyspark.ml.classification import DecisionTreeClassifier # importing decision tree classifier model\n",
    "from pyspark.ml.classification import GBTClassifier # importing GBT classifier model\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator \n",
    "import matplotlib.pyplot as plt # importing matplotlib for plotting\n",
    "import numpy as np # importing numpy for numerical calculations\n",
    "from IPython.display import display\n",
    "from pyspark.sql.functions import sequence ,count,isnull,avg,col,when # importing function's from function library\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics # importing multiclass classification for metric evaluation\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Creating Spark Session and Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 01: Import Spark Session and initialize Spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create entry points to spark\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# We add this line to avoid an error : \"Cannot run multiple SparkContexts at once\". \n",
    "# If there is an existing spark context, we will reuse it instead of creating a new context.\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# local[*]: run Spark locally with as many working processors as logical cores on your machine.\n",
    "# In the field of `master`, we use a local server with as many working processors (or threads) as possible (i.e. `local[*]`). \n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as `local[k]`.\n",
    "# The `appName` field is a name to be shown on the Sparking cluster UI. \n",
    "\n",
    "# If there is no existing spark context, we now create a new context\n",
    "if (sc is None):\n",
    "    sc = SparkContext(master=\"local[4]\", appName=\"Model Building-Assignment2\")\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 02: Load the dataset and print the schema and total number of entries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_US_df  = spark.read.csv('weatherAUS.csv',header=True,inferSchema=True) # reading input csv file using spark method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- MinTemp: string (nullable = true)\n",
      " |-- MaxTemp: string (nullable = true)\n",
      " |-- Rainfall: string (nullable = true)\n",
      " |-- Evaporation: string (nullable = true)\n",
      " |-- Sunshine: string (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: string (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: string (nullable = true)\n",
      " |-- WindSpeed3pm: string (nullable = true)\n",
      " |-- Humidity9am: string (nullable = true)\n",
      " |-- Humidity3pm: string (nullable = true)\n",
      " |-- Pressure9am: string (nullable = true)\n",
      " |-- Pressure3pm: string (nullable = true)\n",
      " |-- Cloud9am: string (nullable = true)\n",
      " |-- Cloud3pm: string (nullable = true)\n",
      " |-- Temp9am: string (nullable = true)\n",
      " |-- Temp3pm: string (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_US_df.printSchema() # schema of our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in the data frame are: 142193\n"
     ]
    }
   ],
   "source": [
    "print('Total records in the data frame are: '+str(weather_US_df.count())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Data Cleaning and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 03: Delete columns from the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns to be dropped : <br>\n",
    "●Date\n",
    "● Location\n",
    "● Evaporation\n",
    "● Sunshine\n",
    "● Cloud9am\n",
    "● Cloud3pm\n",
    "● Temp9am\n",
    "● Temp3pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Date','Location','Evaporation','Sunshine','Cloud9am','Cloud3pm','Temp9am','Temp3pm'] #columns to be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with remaining columns after dropping the drop_list columns\n",
    "weather_US_df_new = weather_US_df.select([column for column in weather_US_df.columns if column not in drop_list]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinTemp: string (nullable = true)\n",
      " |-- MaxTemp: string (nullable = true)\n",
      " |-- Rainfall: string (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: string (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: string (nullable = true)\n",
      " |-- WindSpeed3pm: string (nullable = true)\n",
      " |-- Humidity9am: string (nullable = true)\n",
      " |-- Humidity3pm: string (nullable = true)\n",
      " |-- Pressure9am: string (nullable = true)\n",
      " |-- Pressure3pm: string (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_US_df_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = weather_US_df_new.columns # columns stored and to be used later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 04: Print the number of missing data in each column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "|MinTemp|MaxTemp|Rainfall|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|RainToday|RainTomorrow|\n",
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "|637    |322    |1406    |9330       |9270         |10013     |3778      |1348        |2630        |1774       |3610       |14014      |13981      |1406     |0           |\n",
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# used functions method to count number of 'NA' values \n",
    "weather_US_df_new.select([F.count(F.when(F.col(i).contains('NA'), i)).alias(i) for i in weather_US_df_new.columns]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that except column \"RainTomorrow\" all other's have null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 05: Fill the missing data with average value and maximum occurrence value.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For all the numerical columns we will find the mean and then fill the missing values with the mean**\n",
    "* We have used aggregate function to calculate the mean values for all numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.186399728729311"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mintemp_mean = weather_US_df_new.agg({'MinTemp':'mean'}).collect()[0][0]\n",
    "mintemp_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.2267841912725"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxtemp_mean = weather_US_df_new.agg({'MaxTemp':'mean'}).collect()[0][0]\n",
    "maxtemp_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3499740743107442"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rainfall_mean = weather_US_df_new.agg({'Rainfall':'mean'}).collect()[0][0]\n",
    "rainfall_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.98429165757619"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wind_gust_mean = weather_US_df_new.agg({'WindGustSpeed':'mean'}).collect()[0][0]\n",
    "wind_gust_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.001988000994"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wind_speed9am_mean = weather_US_df_new.agg({'WindSpeed9am':'mean'}).collect()[0][0]\n",
    "wind_speed9am_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.63757586179718"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wind_speed3pm_mean = weather_US_df_new.agg({'WindSpeed3pm':'mean'}).collect()[0][0]\n",
    "wind_speed3pm_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.8438103105705"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humidity_9am_mean = weather_US_df_new.agg({'Humidity9am':'mean'}).collect()[0][0]\n",
    "humidity_9am_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.482606091656265"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humidity_3pm_mean = weather_US_df_new.agg({'Humidity3pm':'mean'}).collect()[0][0]\n",
    "humidity_3pm_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1017.6537584159615"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pressure_9am_mean = weather_US_df_new.agg({'Pressure9am':'mean'}).collect()[0][0]\n",
    "pressure_9am_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1015.2582035378894"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pressure_3pm_mean = weather_US_df_new.agg({'Pressure3pm':'mean'}).collect()[0][0]\n",
    "pressure_3pm_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For all the non numerical columns we will find the most frequent value of a column and then use that value to fill the missing values of that column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|WindGustDir|count|\n",
      "+-----------+-----+\n",
      "|          W| 9780|\n",
      "|         NA| 9330|\n",
      "|         SE| 9309|\n",
      "|          E| 9071|\n",
      "|          N| 9033|\n",
      "|        SSE| 8993|\n",
      "|          S| 8949|\n",
      "|        WSW| 8901|\n",
      "|         SW| 8797|\n",
      "|        SSW| 8610|\n",
      "|        WNW| 8066|\n",
      "|         NW| 8003|\n",
      "|        ENE| 7992|\n",
      "|        ESE| 7305|\n",
      "|         NE| 7060|\n",
      "|        NNW| 6561|\n",
      "|        NNE| 6433|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_US_df_new.groupby('WindGustDir').count().sort('count',ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that the most frequent \"WindGustDir\" value is 'W', which is repeated 9780 times. So we will replace NA values with 'W'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|WindDir9am|count|\n",
      "+----------+-----+\n",
      "|         N|11393|\n",
      "|        NA|10013|\n",
      "|        SE| 9162|\n",
      "|         E| 9024|\n",
      "|       SSE| 8966|\n",
      "|        NW| 8552|\n",
      "|         S| 8493|\n",
      "|         W| 8260|\n",
      "|        SW| 8237|\n",
      "|       NNE| 7948|\n",
      "|       NNW| 7840|\n",
      "|       ENE| 7735|\n",
      "|       ESE| 7558|\n",
      "|        NE| 7527|\n",
      "|       SSW| 7448|\n",
      "|       WNW| 7194|\n",
      "|       WSW| 6843|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_US_df_new.groupby('WindDir9am').count().sort('count',ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that the most frequent \"WindDir9am\" value is 'N', which is repeated 11393 times. So we will replace NA values with 'N'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|WindDir3pm|count|\n",
      "+----------+-----+\n",
      "|        SE|10663|\n",
      "|         W| 9911|\n",
      "|         S| 9598|\n",
      "|       WSW| 9329|\n",
      "|        SW| 9182|\n",
      "|       SSE| 9142|\n",
      "|         N| 8667|\n",
      "|       WNW| 8656|\n",
      "|        NW| 8468|\n",
      "|       ESE| 8382|\n",
      "|         E| 8342|\n",
      "|        NE| 8164|\n",
      "|       SSW| 8010|\n",
      "|       NNW| 7733|\n",
      "|       ENE| 7724|\n",
      "|       NNE| 6444|\n",
      "|        NA| 3778|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_US_df_new.groupby('WindDir3pm').count().sort('count',ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that the most frequent \"WindDir3pm\" value is 'SE', which is repeated 10663 times. So we will replace NA values with 'SE'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|RainToday| count|\n",
      "+---------+------+\n",
      "|       No|109332|\n",
      "|      Yes| 31455|\n",
      "|       NA|  1406|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_US_df_new.groupby('RainToday').count().sort('count',ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that the most frequent \"RainToday\" value is 'No', which is repeated 109332 times. So we will replace NA values with 'No'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling the missing values <br>\n",
    "* First we will fix the numerical columns and replace the 'NA' with their respective means\n",
    "* Next we will fix the non-numerical columns and replace the 'NA' with the most occuring values in respective columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to store all mean values of numerical columns\n",
    "mean_numerical_columns = [mintemp_mean,maxtemp_mean,rainfall_mean,wind_gust_mean,wind_speed9am_mean,wind_speed3pm_mean,humidity_9am_mean,humidity_3pm_mean,pressure_9am_mean,pressure_3pm_mean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_US_df_new[weather_US_df_new['WindGustSpeed']=='NA'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing all the 'NA' values in numerical columns with respective mean values\n",
    "weather_US_df_new = weather_US_df_new.withColumn('MinTemp',when(weather_US_df_new['MinTemp']=='NA',mintemp_mean)\\\n",
    "                             .otherwise(weather_US_df_new['MinTemp'])).withColumn('MaxTemp',when(weather_US_df_new['MaxTemp']=='NA',maxtemp_mean)\\\n",
    "                             .otherwise(weather_US_df_new['MaxTemp'])).withColumn('Rainfall',when(weather_US_df_new['Rainfall']=='NA',rainfall_mean)\\\n",
    "                             .otherwise(weather_US_df_new['Rainfall'])).withColumn('WindGustSpeed',when(weather_US_df_new['WindGustSpeed']=='NA',wind_gust_mean)\\\n",
    "                             .otherwise(weather_US_df_new['WindGustSpeed'])).withColumn('WindSpeed9am',when(weather_US_df_new['WindSpeed9am']=='NA',wind_speed9am_mean)\\\n",
    "                             .otherwise(weather_US_df_new['WindSpeed9am'])).withColumn('WindSpeed3pm',when(weather_US_df_new['WindSpeed3pm']=='NA',wind_speed3pm_mean)\\\n",
    "                             .otherwise(weather_US_df_new['WindSpeed3pm'])).withColumn('Humidity9am',when(weather_US_df_new['Humidity9am']=='NA',humidity_9am_mean)\\\n",
    "                             .otherwise(weather_US_df_new['Humidity9am'])).withColumn('Humidity3pm',when(weather_US_df_new['Humidity3pm']=='NA',humidity_3pm_mean)\\\n",
    "                             .otherwise(weather_US_df_new['Humidity3pm'])).withColumn('Pressure9am',when(weather_US_df_new['Pressure9am']=='NA',pressure_9am_mean)\\\n",
    "                             .otherwise(weather_US_df_new['Pressure9am'])).withColumn('Pressure3pm',when(weather_US_df_new['Pressure3pm']=='NA',pressure_3pm_mean)\\\n",
    "                             .otherwise(weather_US_df_new['Pressure3pm']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing all 'NA' values in string columns with the most frequent values found earlier\n",
    "weather_US_df_new = weather_US_df_new.\\\n",
    "withColumn('WindGustDir',when(weather_US_df_new['WindGustDir']=='NA','W').otherwise(weather_US_df_new['WindGustDir'])).\\\n",
    "withColumn('WindDir9am',when(weather_US_df_new['WindDir9am']=='NA','N').otherwise(weather_US_df_new['WindDir9am'])).\\\n",
    "withColumn('WindDir3pm',when(weather_US_df_new['WindDir3pm']=='NA','SE').otherwise(weather_US_df_new['WindDir3pm'])).\\\n",
    "withColumn('RainToday',when(weather_US_df_new['RainToday']=='NA','No').otherwise(weather_US_df_new['RainToday']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "|MinTemp|MaxTemp|Rainfall|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|RainToday|RainTomorrow|\n",
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "|0      |0      |0       |0          |0            |0         |0         |0           |0           |0          |0          |0          |0          |0        |0           |\n",
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_US_df_new.select([F.count(F.when(F.col(i).contains('NA'), i)).alias(i) for i in weather_US_df_new.columns]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all the missing values have been removed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Step 06: Data transformation**\n",
    "* **Step 07: Create the feature vector and divide the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinTemp: string (nullable = true)\n",
      " |-- MaxTemp: string (nullable = true)\n",
      " |-- Rainfall: string (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: string (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: string (nullable = true)\n",
      " |-- WindSpeed3pm: string (nullable = true)\n",
      " |-- Humidity9am: string (nullable = true)\n",
      " |-- Humidity3pm: string (nullable = true)\n",
      " |-- Pressure9am: string (nullable = true)\n",
      " |-- Pressure3pm: string (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_US_df_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing all numerical column into a list\n",
    "numerical_columns = ['MinTemp','MaxTemp','Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm','Humidity9am','Humidity3pm','Pressure9am','Pressure3pm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting all numerical columns to Double using cast() function\n",
    "for col in numerical_columns:\n",
    "    weather_US_df_new = weather_US_df_new.withColumn(col,weather_US_df_new[col].cast('Double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinTemp: double (nullable = true)\n",
      " |-- MaxTemp: double (nullable = true)\n",
      " |-- Rainfall: double (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: double (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: double (nullable = true)\n",
      " |-- WindSpeed3pm: double (nullable = true)\n",
      " |-- Humidity9am: double (nullable = true)\n",
      " |-- Humidity3pm: double (nullable = true)\n",
      " |-- Pressure9am: double (nullable = true)\n",
      " |-- Pressure3pm: double (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_US_df_new.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use string indexer to convert string columns into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric_columns = ['WindGustDir','WindDir9am','WindDir3pm','RainToday']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting string categorical columns:\n",
    "* First into indexes using StringIndexer(). Naming as **column_index**\n",
    "* Then into vectors using OneHotEncoder.Each categorical indexed columns will be converted into a new vector column. Naming as   **column_encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stages list will store all the objects created while using StringIndexer and OneHotEncoder\n",
    "# each column will create two objects for StringIndexer and OneHotEncoder respectively\n",
    "stages=[] \n",
    "for col in non_numeric_columns: # iterating through non numeric columns\n",
    "    #Convert non numeric values into indexed categories\n",
    "    stringIndexer = StringIndexer(inputCol = col, outputCol = col + '_index') \n",
    "    #Convert indexed categories to one-hot encoded variables (classVec)\n",
    "    encoder = OneHotEncoder(inputCol = stringIndexer.getOutputCol(),outputCol=col+\"_encoder\")\n",
    "    #When printing steps, a binary vector is added to the end of each line.\n",
    "    stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_86ad70e33c6e,\n",
       " OneHotEncoder_c76a3900113c,\n",
       " StringIndexer_dde37061f000,\n",
       " OneHotEncoder_1d34924ae9dc,\n",
       " StringIndexer_17aefb9d2be6,\n",
       " OneHotEncoder_385a92d38c72,\n",
       " StringIndexer_0e49047dc49f,\n",
       " OneHotEncoder_e7615d3f297c]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we convert the \"RainTomorrow\" column, which is our taget or label column into indexes.\n",
    "* We will use StringIndexer() and name the output column as \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_stringIdx = StringIndexer(inputCol = 'RainTomorrow', outputCol = 'label') # indexing our target column\n",
    "stages += [label_stringIdx] # storing the object in stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After string indexing and vectorizing the categorical columns, we need to convert all the numerical columns and encoded categorical string columns into a single vector.\n",
    "* This vector will be passed to the machine learning models we use further\n",
    "* For vectorization we use **VectorAssembler()**\n",
    "* Output vector is named as \"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the encoded features and numerical features into a list, to be used by VectorAssembler()\n",
    "assemblerInputs = [c + \"_encoder\" for c in non_numeric_columns] + numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WindGustDir_encoder',\n",
       " 'WindDir9am_encoder',\n",
       " 'WindDir3pm_encoder',\n",
       " 'RainToday_encoder',\n",
       " 'MinTemp',\n",
       " 'MaxTemp',\n",
       " 'Rainfall',\n",
       " 'WindGustSpeed',\n",
       " 'WindSpeed9am',\n",
       " 'WindSpeed3pm',\n",
       " 'Humidity9am',\n",
       " 'Humidity3pm',\n",
       " 'Pressure9am',\n",
       " 'Pressure3pm']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assemblerInputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing vector assembler\n",
    "# it will convert all the columns into a  single vector feature, to be used by ML algorithms\n",
    "assembler = VectorAssembler(inputCols = assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_86ad70e33c6e,\n",
       " OneHotEncoder_c76a3900113c,\n",
       " StringIndexer_dde37061f000,\n",
       " OneHotEncoder_1d34924ae9dc,\n",
       " StringIndexer_17aefb9d2be6,\n",
       " OneHotEncoder_385a92d38c72,\n",
       " StringIndexer_0e49047dc49f,\n",
       " OneHotEncoder_e7615d3f297c,\n",
       " StringIndexer_09e68e58549d,\n",
       " VectorAssembler_4e5a802746cd]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use **Pipeline()** method to execute all the stages we created earlier\n",
    "* Stages will be executed in the order they are given in  the list\n",
    "* Output of one stage is carried forward and used in the next stage if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = stages) # passing all the stages to the pipeline\n",
    "pipelineModel = pipeline.fit(weather_US_df_new) # fitting/loading the pipeline method with our dataframe\n",
    "weather_US_df_new = pipelineModel.transform(weather_US_df_new) # transforming/executing all the stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinTemp: double (nullable = true)\n",
      " |-- MaxTemp: double (nullable = true)\n",
      " |-- Rainfall: double (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: double (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: double (nullable = true)\n",
      " |-- WindSpeed3pm: double (nullable = true)\n",
      " |-- Humidity9am: double (nullable = true)\n",
      " |-- Humidity3pm: double (nullable = true)\n",
      " |-- Pressure9am: double (nullable = true)\n",
      " |-- Pressure3pm: double (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      " |-- WindGustDir_index: double (nullable = false)\n",
      " |-- WindGustDir_encoder: vector (nullable = true)\n",
      " |-- WindDir9am_index: double (nullable = false)\n",
      " |-- WindDir9am_encoder: vector (nullable = true)\n",
      " |-- WindDir3pm_index: double (nullable = false)\n",
      " |-- WindDir3pm_encoder: vector (nullable = true)\n",
      " |-- RainToday_index: double (nullable = false)\n",
      " |-- RainToday_encoder: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_US_df_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cols = ['label','features'] + cols # final columns that conatin intial columns, label and features\n",
    "df = weather_US_df_new.select(model_cols) # selecting the columns and storing them into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- MinTemp: double (nullable = true)\n",
      " |-- MaxTemp: double (nullable = true)\n",
      " |-- Rainfall: double (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: double (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: double (nullable = true)\n",
      " |-- WindSpeed3pm: double (nullable = true)\n",
      " |-- Humidity9am: double (nullable = true)\n",
      " |-- Humidity3pm: double (nullable = true)\n",
      " |-- Pressure9am: double (nullable = true)\n",
      " |-- Pressure3pm: double (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(56,[0,21,37,45,4...|  0.0|\n",
      "|(56,[9,24,33,45,4...|  0.0|\n",
      "|(56,[6,21,33,45,4...|  0.0|\n",
      "|(56,[13,16,40,45,...|  0.0|\n",
      "|(56,[0,25,38,45,4...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('features','label').show(5) # displaying our label and the feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the feature vector and label(taget column) in our data frame. Next step is to divide the dataset into train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 99626\n",
      "Test Dataset Count: 42567\n"
     ]
    }
   ],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3],seed = 2018) # splitting our data into 70% and 30% for training and testing respectively \n",
    "print(\"Training Dataset Count: \" + str(train.count())) # count of training data\n",
    "print(\"Test Dataset Count: \" + str(test.count())) # count of testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Apply Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 08: Apply machine learning classification algorithms on the dataset and\n",
    "compare their accuracy. Plot the accuracy as bar graph**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing features into the logistic regression model and fitting the object with the train data\n",
    "log_reg = LogisticRegression(featuresCol = 'features', labelCol = 'label',maxIter=10)\n",
    "log_model = log_reg.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using transform() method and our test data to generate our predictions\n",
    "predictions_log  = log_model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilities of RainTomorrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|probability                             |\n",
      "+----------------------------------------+\n",
      "|[0.8385150282965601,0.16148497170343995]|\n",
      "|[0.8142321374621219,0.18576786253787805]|\n",
      "|[0.8307788030407406,0.16922119695925944]|\n",
      "|[0.6571792607547531,0.3428207392452469] |\n",
      "+----------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#probabilities for each selected forecasting class\n",
    "predictions_log.select('probability').show(4,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model\n",
    "* Here we have used BinaryClassificationEvaluator() for calculating Area Under ROC\n",
    "* MulticlassMetrics() to calculate accuarcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_log = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC 0.8151167718867015\n"
     ]
    }
   ],
   "source": [
    "print('Test Area Under ROC', evaluator_log.evaluate(predictions_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting label and prediction columns into a single rdd. This is done so as to give it as input to MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionRDD_log = predictions_log.select(['label', 'prediction']) \\\n",
    "                            .rdd.map(lambda line: (line[1], line[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0), (0.0, 0.0)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionRDD_log.take(2) # predicted values in rdd form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_log = MulticlassMetrics(predictionRDD_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8193201306176146"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_log = metrics_log.accuracy\n",
    "accuracy_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the number of nodes in the decision tree and the tree depth in the model and stores it in dt.\n",
    "d_tree = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 30)\n",
    "tree_model = d_tree.fit(train)\n",
    "#Provisions and probabilities for each selected forecasting class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the  Transformer.transform() method to predict test data\n",
    "d_tree_predictions = tree_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         probability|\n",
      "+--------------------+\n",
      "|[0.91636363636363...|\n",
      "|           [0.0,1.0]|\n",
      "|[0.91636363636363...|\n",
      "|           [1.0,0.0]|\n",
      "+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#probabilities for each selected forecasting class\n",
    "d_tree_predictions.select('probability').show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_tree = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC 0.7043673295845813\n"
     ]
    }
   ],
   "source": [
    "print('Test Area Under ROC', evaluator_tree.evaluate(d_tree_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting label and prediction columns into a single rdd. This is done so as to give it as input to MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionRDD_tree = d_tree_predictions.select(['label', 'prediction']) \\\n",
    "                            .rdd.map(lambda line: (line[1], line[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0), (1.0, 0.0)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionRDD_tree.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_tree = MulticlassMetrics(predictionRDD_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7828364695656259"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_tree = metrics_tree.accuracy\n",
    "accuracy_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RANDOM FOREST**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_forest = RandomForestClassifier(labelCol=\"label\",featuresCol=\"features\", numTrees=5,maxDepth=20,maxBins=60)\n",
    "\n",
    "forest_model = r_forest.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_predictions = forest_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         probability|\n",
      "+--------------------+\n",
      "|[0.96128095271190...|\n",
      "|[0.66333333333333...|\n",
      "|[0.91528792135301...|\n",
      "|[0.70308138201233...|\n",
      "+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#probabilities for each selected forecasting class\n",
    "forest_predictions.select('probability').show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_forest = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC 0.8368546138734495\n"
     ]
    }
   ],
   "source": [
    "print('Test Area Under ROC', evaluator_forest.evaluate(forest_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting label and prediction columns into a single rdd. This is done so as to give it as input to MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionRDD_forest = forest_predictions.select(['label', 'prediction']) \\\n",
    "                            .rdd.map(lambda line: (line[1], line[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0), (0.0, 0.0)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionRDD_forest.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_forest = MulticlassMetrics(predictionRDD_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8342847745906453"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_forest = metrics_forest.accuracy\n",
    "accuracy_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GBT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(maxIter=10)\n",
    "gbt_model = gbt.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_predictions = gbt_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         probability|\n",
      "+--------------------+\n",
      "|[0.86440638933661...|\n",
      "|[0.90055937305706...|\n",
      "|[0.86296668205049...|\n",
      "|[0.72750979599664...|\n",
      "+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#probabilities for each selected forecasting class\n",
    "gbt_predictions.select('probability').show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_gbt = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC 0.8546122574102634\n"
     ]
    }
   ],
   "source": [
    "print('Test Area Under ROC', evaluator_gbt.evaluate(gbt_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting label and prediction columns into a single rdd. This is done so as to give it as input to MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionRDD_gbt = gbt_predictions.select(['label', 'prediction']) \\\n",
    "                            .rdd.map(lambda line: (line[1], line[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0), (0.0, 0.0)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionRDD_gbt.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_gbt = MulticlassMetrics(predictionRDD_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8421547207931026"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_gbt = metrics_gbt.accuracy\n",
    "accuracy_gbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(x=['Logistic Regression','Decision Tree','Random Forest','GBT'],height=[accuracy_log,accuracy_tree,accuracy_forest,accuracy_gbt],\\\n",
    "        alpha=1,align='center',color='rbgy')\n",
    "plt.xticks(rotation=10)\n",
    "plt.xlabel('Machine Learning Model\\'s')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 09: Calculate the confusion matrix and find the precision, recall, and F1\n",
    "score of each classification algorithm. Explain how the accuracy of the\n",
    "predication can be improved?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the metric's object's created for each of the model's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METRICS FOR LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_log = metrics_log.confusionMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30838.  2220.]\n",
      " [ 5471.  4038.]]\n"
     ]
    }
   ],
   "source": [
    "print(cm_log.toArray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision, Recall, and F1 score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for Logistic Regression is : 0.42465033126511725\n",
      "Precision for Logistic Regression is : 0.6452540747842761\n",
      "F1 Measure for Logistic Regression is : 0.5122090442062535\n"
     ]
    }
   ],
   "source": [
    "TN_Log = cm_log.values[0]\n",
    "TP_log = cm_log.values[3]\n",
    "FN_log = cm_log.values[1]\n",
    "FP_log = cm_log.values[2]\n",
    "Recall_log = TP_log/(TP_log+FN_log)\n",
    "Precision_log = TP_log/(TP_log+FP_log)\n",
    "F1_log = (2 * Precision_log * Recall_log)/(Precision_log+Recall_log)\n",
    "print(\"Recall for Logistic Regression is : %s\" % Recall_log )\n",
    "print(\"Precision for Logistic Regression is : %s\" % Precision_log)\n",
    "print(\"F1 Measure for Logistic Regression is : %s\" % F1_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weighted Precision, Recall, and F1 score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted recall = 0.8193201306176146\n",
      "Weighted precision = 0.8037347730612462\n",
      "Weighted F(1) Score = 0.8049268579962912\n",
      "Weighted F(0.0) Score = 0.8037347730612462\n",
      "Weighted false positive rate = 0.4618244971563572\n"
     ]
    }
   ],
   "source": [
    "print(\"Weighted recall = %s\" % metrics_log.weightedRecall)\n",
    "print(\"Weighted precision = %s\" % metrics_log.weightedPrecision)\n",
    "print(\"Weighted F(1) Score = %s\" % metrics_log.weightedFMeasure())\n",
    "print(\"Weighted F(0.0) Score = %s\" % metrics_log.weightedFMeasure(0.0))\n",
    "print(\"Weighted false positive rate = %s\" % metrics_log.weightedFalsePositiveRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METRICS FOR DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_tree = metrics_tree.confusionMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28271.  4787.]\n",
      " [ 4457.  5052.]]\n"
     ]
    }
   ],
   "source": [
    "print(cm_tree.toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for DT is : 0.5312861499631928\n",
      "Precision for DT is : 0.5134668157333062\n",
      "F1 Measure for DT is : 0.5222245193301632\n"
     ]
    }
   ],
   "source": [
    "TN_tree = cm_tree.values[0]\n",
    "TP_tree = cm_tree.values[3]\n",
    "FN_tree = cm_tree.values[1]\n",
    "FP_tree = cm_tree.values[2]\n",
    "Recall_tree = TP_tree/(TP_tree+FN_tree)\n",
    "Precision_tree = TP_tree/(TP_tree+FP_tree)\n",
    "F1_tree = (2 * Precision_tree * Recall_tree)/(Precision_tree+Recall_tree)\n",
    "print(\"Recall for DT is : %s\" % Recall_tree )\n",
    "print(\"Precision for DT is : %s\" % Precision_tree)\n",
    "print(\"F1 Measure for DT is : %s\" % F1_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted recall = 0.7828364695656259\n",
      "Weighted precision = 0.7855525532176872\n",
      "Weighted F(1) Score = 0.7841437681333714\n",
      "Weighted F(0.0) Score = 0.7855525532176872\n",
      "Weighted false positive rate = 0.3963564179749905\n"
     ]
    }
   ],
   "source": [
    "print(\"Weighted recall = %s\" % metrics_tree.weightedRecall)\n",
    "print(\"Weighted precision = %s\" % metrics_tree.weightedPrecision)\n",
    "print(\"Weighted F(1) Score = %s\" % metrics_tree.weightedFMeasure())\n",
    "print(\"Weighted F(0.0) Score = %s\" % metrics_tree.weightedFMeasure(0.0))\n",
    "print(\"Weighted false positive rate = %s\" % metrics_tree.weightedFalsePositiveRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METRICS FOR GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_gbt = metrics_gbt.confusionMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[31411.,  1647.],\n",
       "       [ 5072.,  4437.]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_gbt.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for GBT is : 0.4666105794510464\n",
      "Precision for GBT is : 0.7292899408284024\n",
      "F1 Measure for GBT is : 0.5691015199127814\n"
     ]
    }
   ],
   "source": [
    "TN_gbt = cm_gbt.values[0]\n",
    "TP_gbt =  cm_gbt.values[3]\n",
    "FN_gbt = cm_gbt.values[1]\n",
    "FP_gbt = cm_gbt.values[2]\n",
    "Recall_gbt = TP_gbt/(TP_gbt+FN_gbt)\n",
    "Precision_gbt = TP_gbt/(TP_gbt+FP_gbt)\n",
    "F1_gbt = (2 * Precision_gbt * Recall_gbt)/(Precision_gbt+Recall_gbt)\n",
    "print(\"Recall for GBT is : %s\" % Recall_gbt )\n",
    "print(\"Precision for GBT is : %s\" % Precision_gbt)\n",
    "print(\"F1 Measure for GBT is : %s\" % F1_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted recall = 0.8421547207931026\n",
      "Weighted precision = 0.8315590496208005\n",
      "Weighted F(1) Score = 0.8287064354216658\n",
      "Weighted F(0.0) Score = 0.8315590496208005\n",
      "Weighted false positive rate = 0.42536566714519014\n"
     ]
    }
   ],
   "source": [
    "print(\"Weighted recall = %s\" % metrics_gbt.weightedRecall)\n",
    "print(\"Weighted precision = %s\" % metrics_gbt.weightedPrecision)\n",
    "print(\"Weighted F(1) Score = %s\" % metrics_gbt.weightedFMeasure())\n",
    "print(\"Weighted F(0.0) Score = %s\" % metrics_gbt.weightedFMeasure(0.0))\n",
    "print(\"Weighted false positive rate = %s\" % metrics_gbt.weightedFalsePositiveRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_forest = metrics_forest.confusionMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for Random Forest is : 0.4864864864864865\n",
      "Precision for Random Forest is : 0.6805943798734736\n",
      "F1 Measure for Random Forest is : 0.5673985036183\n"
     ]
    }
   ],
   "source": [
    "TN_forest = cm_forest.values[0]\n",
    "TP_forest = cm_forest.values[3]\n",
    "FN_forest = cm_forest.values[1]\n",
    "FP_forest = cm_forest.values[2]\n",
    "Recall_forest = TP_forest/(TP_forest+FN_forest)\n",
    "Precision_forest = TP_forest/(TP_forest+FP_forest)\n",
    "F1_forest = (2 * Precision_forest * Recall_forest)/(Precision_forest+Recall_forest)\n",
    "print(\"Recall for Random Forest is : %s\" % Recall_forest )\n",
    "print(\"Precision for Random Forest is : %s\" % Precision_forest)\n",
    "print(\"F1 Measure for Random Forest is : %s\" % F1_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted recall = 0.8342847745906454\n",
      "Weighted precision = 0.8226323186087341\n",
      "Weighted F(1) Score = 0.8237687699576907\n",
      "Weighted F(0.0) Score = 0.8226323186087341\n",
      "Weighted false positive rate = 0.4134707425781137\n"
     ]
    }
   ],
   "source": [
    "print(\"Weighted recall = %s\" % metrics_forest.weightedRecall)\n",
    "print(\"Weighted precision = %s\" % metrics_forest.weightedPrecision)\n",
    "print(\"Weighted F(1) Score = %s\" % metrics_forest.weightedFMeasure())\n",
    "print(\"Weighted F(0.0) Score = %s\" % metrics_forest.weightedFMeasure(0.0))\n",
    "print(\"Weighted false positive rate = %s\" % metrics_forest.weightedFalsePositiveRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy of the Model can be improved based on the following points -**\n",
    "- Our dataset is a little biased as most of the label values are 0. This creates a biased response for all the models. A dataset were the label is balanced towards both 0 and 1 categories will result in much better predictions and hence accuracy.\n",
    "- We could have handled the missing values in a much better way, rather than just filling them with most frequent and mean values.A correlation trend between features could give us a better understanding of how to fill up the missing values. \n",
    "- We have imputed the missing values in the dataset, but we have not handled the probable outliers in the data. As we have numerical data, there is possibility of Outliers to be present in the dataset. Presence of outliers will affect the accuracy of the predictive model . Hence it is important to remove the outliers.\n",
    "- Whenever we select the independent variables that is the features , we should see the correlation between the dependent variable and features . features should be selected in such a way that Correlation between features and predictive label is high.\n",
    "- Normalization of Data should be done.\n",
    "- Parameter tuning can be done to improve the accuracy. \n",
    "- Ensemble methods can be applied to the models to combine results of weak models and improve accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
